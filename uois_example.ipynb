{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unseen Object Instance Segmentation\n",
    "\n",
    "In tabletop environments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import json\n",
    "from time import time\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.io\n",
    "import cv2\n",
    "\n",
    "# My libraries\n",
    "import src.data_loader as data_loader\n",
    "import src.data_augmentation as data_augmentation\n",
    "import src.segmentation as segmentation\n",
    "import src.evaluation as evaluation\n",
    "import src.util.utilities as util_\n",
    "import src.util.flowlib as flowlib\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\" # TODO: Change this if you have more than 1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_to_numpy(torch_tensor, is_standardized_image = False):\n",
    "    \"\"\" Converts torch tensor (NCHW) to numpy tensor (NHWC) for plotting\n",
    "    \n",
    "        If it's an rgb image, it puts it back in [0,255] range (and undoes ImageNet standardization)\n",
    "    \"\"\"\n",
    "    np_tensor = torch_tensor.cpu().clone().detach().numpy()\n",
    "    if np_tensor.ndim == 4: # NCHW\n",
    "        np_tensor = np_tensor.transpose(0,2,3,1)\n",
    "    if is_standardized_image:\n",
    "        _mean=[0.485, 0.456, 0.406]; _std=[0.229, 0.224, 0.225]\n",
    "        for i in range(3):\n",
    "            np_tensor[...,i] *= _std[i]\n",
    "            np_tensor[...,i] += _mean[i]\n",
    "        np_tensor *= 255\n",
    "            \n",
    "    return np_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth Seeding Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsn_params = {\n",
    "    \n",
    "    # Sizes\n",
    "    'feature_dim' : 64,\n",
    "    \n",
    "    # algorithm parameters\n",
    "    'lr' : 1e-2, # learning rate\n",
    "    'iter_collect' : 20, # Collect results every _ iterations\n",
    "    'max_iters' : 100000,\n",
    "    \n",
    "    # architecture parameters\n",
    "    'use_coordconv' : False,\n",
    "\n",
    "    # Loss function parameters\n",
    "    'lambda_fg' : 1,\n",
    "    'lambda_direction' : 1.,\n",
    "\n",
    "    # Hough Voting parameters\n",
    "    'skip_pixels' : 10, \n",
    "    'inlier_threshold' : 0.9, \n",
    "    'angle_discretization' : 100,\n",
    "    'inlier_distance' : 20,\n",
    "    'percentage_threshold' : 0.5, # this depends on skip_pixels, angle_discretization, inlier_distance. just gotta try it to see if it works\n",
    "    'object_center_kernel_radius' : 10,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region Refinement Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrn_params = {\n",
    "    \n",
    "    # Sizes\n",
    "    'feature_dim' : 64,\n",
    "    \n",
    "    # algorithm parameters\n",
    "    'lr' : 1e-2, # learning rate\n",
    "    'iter_collect' : 20, # Collect results every _ iterations\n",
    "    'max_iters' : 100000,\n",
    "    \n",
    "    # architecture parameters\n",
    "    'use_coordconv' : False,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabletop Segmentor parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_params = {\n",
    "    \n",
    "    # Padding for Region Refinement Network\n",
    "    'padding_percentage' : 0.25,\n",
    "    \n",
    "    # Open/Close Morphology for IMP (Initial Mask Processing) module\n",
    "    'use_open_close_morphology' : True,\n",
    "    'open_close_morphology_ksize' : 9,\n",
    "    \n",
    "    # Closest Connected Component for IMP module\n",
    "    'use_closest_connected_component' : True,\n",
    "    \n",
    "}\n",
    "checkpoint_dir = '...' # TODO: change this to directory of downloaded models\n",
    "dsn_filename = checkpoint_dir + 'DepthSeedingNetwork_TOD_checkpoint.pth'\n",
    "rrn_filename = checkpoint_dir + 'RRN_TOD_checkpoint.pth'\n",
    "tts_params['final_close_morphology'] = 'TOD' in rrn_filename\n",
    "tabletop_segmentor = segmentation.TableTopSegmentor(tts_params, \n",
    "                                                    dsn_filename,\n",
    "                                                    dsn_params,\n",
    "                                                    rrn_filename,\n",
    "                                                    rrn_params\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on example images\n",
    "\n",
    "We provided some example RGB-D images of scenarios in our lab environments. The following code loads those images and runs the network on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_images_dir = os.path.abspath('.') + '/example_images/'\n",
    "image_files = sorted(glob.glob(example_images_dir + '/image_*.npy'))\n",
    "N = len(image_files)\n",
    "\n",
    "camera_params = json.load(open(example_images_dir + 'camera_params.json'))\n",
    "\n",
    "rgb_imgs = np.zeros((N, 480, 640, 3), dtype=np.float32)\n",
    "xyz_imgs = np.zeros((N, 480, 640, 3), dtype=np.float32)\n",
    "for i, img_file in enumerate(image_files):\n",
    "    d = np.load(img_file, allow_pickle=True, encoding='bytes').item()\n",
    "    \n",
    "    # RGB\n",
    "    rgb_img = d['rgb']\n",
    "    rgb_imgs[i] = data_augmentation.standardize_image(rgb_img)\n",
    "\n",
    "    # Depth\n",
    "    depth_img = d['depth']\n",
    "    depth_img = (depth_img / 1000.).astype(np.float32) # millimeters -> meters\n",
    "\n",
    "    # Compute xyz ordered point cloud\n",
    "    xyz_img = data_loader.compute_xyz(depth_img,camera_params)\n",
    "    xyz_imgs[i] = xyz_img\n",
    "    \n",
    "batch = {\n",
    "    'rgb' : data_augmentation.array_to_tensor(rgb_imgs),\n",
    "    'xyz' : data_augmentation.array_to_tensor(xyz_imgs),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of images: {0}\".format(N))\n",
    "\n",
    "### Compute segmentation masks ###\n",
    "st_time = time()\n",
    "fg_masks, direction_predictions, initial_masks, seg_masks = tabletop_segmentor.run_on_batch(batch)\n",
    "total_time = time() - st_time\n",
    "print('Total time taken for Segmentation: {0} seconds'.format(round(total_time, 3)))\n",
    "print('FPS: {0}'.format(round(N / total_time,3)))\n",
    "\n",
    "# Get results in numpy\n",
    "seg_masks = seg_masks.cpu().numpy()\n",
    "fg_masks = fg_masks.cpu().numpy()\n",
    "direction_predictions = direction_predictions.cpu().numpy().transpose(0,2,3,1)\n",
    "initial_masks = initial_masks.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rgb_imgs = torch_to_numpy(batch['rgb'].cpu(), is_standardized_image=True)\n",
    "total_subplots = 6\n",
    "\n",
    "fig_index = 1\n",
    "for i in range(N):\n",
    "    \n",
    "    fig = plt.figure(fig_index); fig_index += 1\n",
    "    fig.set_size_inches(20,5)\n",
    "\n",
    "    # Plot image\n",
    "    plt.subplot(1,total_subplots,1)\n",
    "    plt.imshow(rgb_imgs[i,...].astype(np.uint8))\n",
    "    plt.title('Image {0}'.format(i+1))\n",
    "\n",
    "    # Plot Depth\n",
    "    plt.subplot(1,total_subplots,2)\n",
    "    plt.imshow(xyz_imgs[i,...,2])\n",
    "    plt.title('Depth')\n",
    "    \n",
    "    # Plot initial table mask\n",
    "    plt.subplot(1,total_subplots,3)\n",
    "    plt.imshow(util_.get_color_mask(fg_masks[i,...]))\n",
    "    plt.title(\"Foreground Table Mask\")\n",
    "    \n",
    "    # Plot direction predictions\n",
    "    plt.subplot(1,total_subplots,4)\n",
    "    plt.imshow(flowlib.flow_to_image(direction_predictions[i,...]))\n",
    "    plt.title(\"Center Direction Predictions\")\n",
    "    \n",
    "    # Plot initial masks\n",
    "    plt.subplot(1,total_subplots,5)\n",
    "    plt.imshow(util_.get_color_mask(initial_masks[i,...]))\n",
    "    plt.title(f\"Initial Masks. #objects: {np.unique(initial_masks[i,...]).shape[0]-1}\")\n",
    "    \n",
    "    # Plot Masks\n",
    "    plt.subplot(1,total_subplots,6)\n",
    "    plt.imshow(util_.get_color_mask(seg_masks[i,...]))\n",
    "    plt.title(f\"Refined Masks. #objects: {np.unique(seg_masks[i,...]).shape[0]-1}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on example OSD/OCID images\n",
    "\n",
    "We provide a few [OSD](https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/osd/) and [OCID](https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/object-clutter-indoor-dataset/) images and run the network on them. Evaluation metrics are shown for each of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_images_dir = os.path.abspath('.') + '/example_images/'\n",
    "\n",
    "OSD_image_files = sorted(glob.glob(example_images_dir + '/OSD_*.npy'))\n",
    "OCID_image_files = sorted(glob.glob(example_images_dir + '/OCID_*.npy'))\n",
    "N = len(OSD_image_files) + len(OCID_image_files)\n",
    "\n",
    "rgb_imgs = np.zeros((N, 480, 640, 3), dtype=np.float32)\n",
    "xyz_imgs = np.zeros((N, 480, 640, 3), dtype=np.float32)\n",
    "label_imgs = np.zeros((N, 480, 640), dtype=np.uint8)\n",
    "\n",
    "for i, img_file in enumerate(OSD_image_files + OCID_image_files):\n",
    "    d = np.load(img_file, allow_pickle=True, encoding='bytes').item()\n",
    "    \n",
    "    # RGB\n",
    "    rgb_img = d['rgb']\n",
    "    rgb_imgs[i] = data_augmentation.standardize_image(rgb_img)\n",
    "\n",
    "    # XYZ\n",
    "    xyz_imgs[i] = d['xyz']\n",
    "\n",
    "    # Label\n",
    "    label_imgs[i] = d['label']\n",
    "    \n",
    "batch = {\n",
    "    'rgb' : data_augmentation.array_to_tensor(rgb_imgs),\n",
    "    'xyz' : data_augmentation.array_to_tensor(xyz_imgs),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of images: {0}\".format(N))\n",
    "\n",
    "### Compute segmentation masks ###\n",
    "st_time = time()\n",
    "fg_masks, direction_predictions, initial_masks, seg_masks = tabletop_segmentor.run_on_batch(batch)\n",
    "total_time = time() - st_time\n",
    "print('Total time taken for Segmentation: {0} seconds'.format(round(total_time, 3)))\n",
    "print('FPS: {0}'.format(round(N / total_time,3)))\n",
    "\n",
    "# Get results in numpy\n",
    "seg_masks = seg_masks.cpu().numpy()\n",
    "fg_masks = fg_masks.cpu().numpy()\n",
    "direction_predictions = direction_predictions.cpu().numpy().transpose(0,2,3,1)\n",
    "initial_masks = initial_masks.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_imgs = torch_to_numpy(batch['rgb'].cpu(), is_standardized_image=True)\n",
    "total_subplots = 4\n",
    "\n",
    "fig_index = 1\n",
    "for i in range(N):\n",
    "    \n",
    "    fig = plt.figure(fig_index); fig_index += 1\n",
    "    fig.set_size_inches(20,5)\n",
    "\n",
    "    # Plot image\n",
    "    plt.subplot(1,total_subplots,1)\n",
    "    plt.imshow(rgb_imgs[i,...].astype(np.uint8))\n",
    "    plt.title(f\"Image {i+1}\")\n",
    "\n",
    "    # Plot Depth\n",
    "    plt.subplot(1,total_subplots,2)\n",
    "    plt.imshow(xyz_imgs[i,...,2])\n",
    "    plt.title('Depth')\n",
    "    \n",
    "    num_objs = max(np.unique(seg_masks[i,...]).max(), np.unique(label_imgs[i,...]).max()) + 1\n",
    "    \n",
    "    # Plot Predicted Masks\n",
    "    plt.subplot(1,total_subplots,3)\n",
    "    plt.imshow(util_.get_color_mask(seg_masks[i,...], nc=num_objs))\n",
    "    plt.title(f\"Predicted Masks. #objects: {np.unique(seg_masks[i,...]).shape[0]-1}\")\n",
    "    \n",
    "    # Plot GT Masks\n",
    "    plt.subplot(1,total_subplots,4)\n",
    "    plt.imshow(util_.get_color_mask(label_imgs[i,...], nc=num_objs))\n",
    "    plt.title(f\"Ground Truth. #objects: {np.unique(label_imgs[i,...]).shape[0]-1}\")\n",
    "    \n",
    "    # Run evaluation metric\n",
    "    eval_metrics = evaluation.multilabel_metrics(seg_masks[i,...], label_imgs[i])\n",
    "    print(f\"Image {i+1} Metrics:\")\n",
    "    print(eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: table label is not considered in evaluation metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ssc]",
   "language": "python",
   "name": "conda-env-ssc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
